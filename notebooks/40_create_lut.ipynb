{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-30T12:06:03.509909Z",
     "start_time": "2025-01-30T12:06:03.469903Z"
    }
   },
   "source": [
    "# This script should take the em_centroids h5_file and query the agglo_id\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tifffile\n",
    "import napari\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "import dill\n",
    "from google.cloud import bigquery\n",
    "from google.auth import exceptions\n",
    "\n",
    "# Custom imports\n",
    "from brainmaps_api_fcn.equivalence_requests import EquivalenceRequests\n",
    "from brainmaps_api_fcn.subvolume_requests import SubvolumeRequest\n",
    "from scripts.sample_db import SampleDB\n",
    "from scripts.utils.image_utils import load_tiff_as_hyperstack"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T12:06:06.450933Z",
     "start_time": "2025-01-30T12:06:06.427925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_agglo_group_with_retry(sa, volume_id, stack_change, centroid_xyz, max_retries=5):\n",
    "    \"\"\"Get agglomeration group with retry mechanism.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            sr = SubvolumeRequest(sa, volume_id)\n",
    "            vol = sr.get_subvolume(centroid_xyz, size=[1,1,1], change_stack_id=stack_change)\n",
    "            agglo_id = int(np.unique(vol[vol>0])[0])\n",
    "            \n",
    "            er = EquivalenceRequests(sa, volume_id, stack_change)\n",
    "            return er.get_groups(agglo_id)\n",
    "        except exceptions.RefreshError:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            print(f\"Try {attempt+1}/{max_retries}\")\n",
    "            time.sleep(attempt)\n",
    "\n",
    "\n",
    "def get_neuron_segments(lut_path, neuron_id):\n",
    "    \"\"\"Get all segment IDs for a given neuron\"\"\"\n",
    "    with h5py.File(lut_path, 'r') as f:\n",
    "        neuron_path = f'neurons/neuron_{neuron_id}/segments'\n",
    "        if neuron_path not in f:\n",
    "            return []\n",
    "\n",
    "        # Get all segment IDs from the segments group\n",
    "        segments = list(f[neuron_path].keys())\n",
    "        # Convert segment IDs from strings to integers\n",
    "        segments = [int(seg) for seg in segments]\n",
    "\n",
    "        return segments\n",
    "\n",
    "\n",
    "def get_segments_by_agglo_id(lut_path, agglo_id):\n",
    "    \"\"\"Get all segment IDs for a given agglomeration ID\"\"\"\n",
    "    segments = []\n",
    "    with h5py.File(lut_path, 'r') as f:\n",
    "        # Iterate through all neurons to find matching agglo_id\n",
    "        for neuron_name, neuron_group in f['neurons'].items():\n",
    "            if 'agglo_id' in neuron_group.attrs:\n",
    "                if neuron_group.attrs['agglo_id'] == agglo_id:\n",
    "                    # Get segments from this neuron\n",
    "                    if 'segments' in neuron_group:\n",
    "                        segments = list(neuron_group['segments'].keys())\n",
    "                        # Convert segment IDs from strings to integers\n",
    "                        segments = [int(seg) for seg in segments]\n",
    "                        break\n",
    "    return segments\n",
    "\n",
    "\n",
    "def find_neurons_in_mask(lut_path, mask_path):\n",
    "    \"\"\"Find neurons with centroids inside a 3d mask\"\"\"\n",
    "    # Open the Paintera zarr mask\n",
    "    mask = tifffile.imread(mask_path, mode='r')\n",
    "    neurons_inside = []\n",
    "\n",
    "    with h5py.File(lut_path, 'r') as f:\n",
    "        for neuron_name, neuron_group in f['neurons'].items():\n",
    "            # Get the neuroglancer coordinates of the centroid\n",
    "            # We use ng coordinates since Paintera mask is in the same space\n",
    "            centroid = neuron_group['em_centroid_ng'][:] // 16\n",
    "\n",
    "            print(centroid)\n",
    "\n",
    "            # Round coordinates to integers for indexing\n",
    "            x, y, z = np.round(centroid).astype(int)\n",
    "\n",
    "            # Check if the centroid is within mask bounds\n",
    "            if (0 <= x < mask.shape[2] and\n",
    "                    0 <= y < mask.shape[1] and\n",
    "                    0 <= z < mask.shape[0]):\n",
    "\n",
    "                # Check if the point is inside the mask (non-zero value)\n",
    "                if mask[x, y, z] > 0:\n",
    "                    neuron_id = int(neuron_name.split('_')[1])\n",
    "                    agglo_id = neuron_group.attrs.get('agglo_id', None)\n",
    "                    neurons_inside.append({\n",
    "                        'neuron_id': neuron_id,\n",
    "                        'agglo_id': agglo_id,\n",
    "                        'centroid': centroid\n",
    "                    })\n",
    "\n",
    "    return neurons_inside\n",
    "\n",
    "\n",
    "def get_neurons_with_attribute(lut_path, attribute_name, attribute_value, operator=\"==\"):\n",
    "    \"\"\"\n",
    "    Get neurons where attribute matches the comparison with threshold\n",
    "    \n",
    "    Args:\n",
    "        lut_path: Path to HDF5 file\n",
    "        attribute_name: Name of attribute to check\n",
    "        operator: String specifying comparison ('>', '<', '>=', '<=', '==')\n",
    "        threshold: Value to compare against\n",
    "    \"\"\"\n",
    "    neuron_with_attribute = []\n",
    "    operators = {\n",
    "        '>': lambda x, y: x > y,\n",
    "        '<': lambda x, y: x < y,\n",
    "        '>=': lambda x, y: x >= y,\n",
    "        '<=': lambda x, y: x <= y,\n",
    "        '==': lambda x, y: x == y\n",
    "    }\n",
    "\n",
    "    if operator not in operators:\n",
    "        raise ValueError(f\"Operator must be one of {list(operators.keys())}\")\n",
    "\n",
    "    with h5py.File(lut_path, 'r') as f:\n",
    "        for neuron, neuron_group in f['neurons'].items():\n",
    "            if attribute_name in neuron_group.attrs:\n",
    "                stored_value = neuron_group.attrs[attribute_name]\n",
    "                if operators[operator](stored_value, attribute_value) and \"agglo_id\" in neuron_group.attrs:\n",
    "                    print(stored_value)\n",
    "                    neuron_with_attribute.append(neuron_group.attrs[\"agglo_id\"])\n",
    "                    #print(f\"Neuron {neuron}: {attribute_name} = {stored_value}\")\n",
    "\n",
    "    return neuron_with_attribute"
   ],
   "id": "2e8b33619301dc11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T12:06:34.733776Z",
     "start_time": "2025-01-30T12:06:32.363671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = bigquery.Client(project=\"aggloproofreading\")\n",
    "\n",
    "# Step 1: Load the sample database\n",
    "db_path = r'\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\sample_db.csv'\n",
    "sample_db = SampleDB()\n",
    "sample_db.load(db_path)\n",
    "# Step 2: Load experiment configuration\n",
    "sample_id = '20220426_RM0008_130hpf_fP1_f3'\n",
    "exp = sample_db.get_sample(sample_id)\n",
    "\n",
    "# TODO: add paths to sample\n",
    "# Input: volume data\n",
    "sa = r\"\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\Reconstruction\\fmi-friedrich-ebb67584e50d.json\"\n",
    "volume_id = r\"280984173682:montano_rm2_ngff:raw_230701_seg_240316fb\"\n",
    "stack_change = \"240705d_rsg9_spl\""
   ],
   "id": "2b4052031234400e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T12:06:36.605788Z",
     "start_time": "2025-01-30T12:06:36.600787Z"
    }
   },
   "cell_type": "code",
   "source": "em_centroids_path = os.path.join(exp.paths.em_path, f\"{exp.sample.id}_em_centroids.h5\")\n",
   "id": "2e5b628425194ebd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T12:06:57.339106Z",
     "start_time": "2025-01-30T12:06:57.299106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read centroids from HDF5 file and do transformation to neuroglancer\n",
    "with h5py.File(em_centroids_path, 'r') as f:\n",
    "    # Get centroids coordinates\n",
    "    data = f['centroids/coordinates'][:]\n",
    "    centroids = data[:, 1:]\n",
    "\n",
    "    # Get transformation parameters\n",
    "    transformation_matrix = f['metadata/transformations/bigwarp2neuroglancer/transformation_matrix'][:]\n",
    "    rotated_cropped_stack_center_shift = f[\n",
    "                                             'metadata/transformations/bigwarp2neuroglancer/rotated_cropped_stack_center_shift'][\n",
    "                                         :]\n",
    "    rotated_stack_center_shift = f['metadata/transformations/bigwarp2neuroglancer/rotated_stack_center_shift'][:]\n",
    "    cropped_shift = f['metadata/transformations/bigwarp2neuroglancer/cropped_shift'][:]\n",
    "    ng_shift = f['metadata/transformations/bigwarp2neuroglancer/ng_shift'][:]\n",
    "    downsampled_factor = f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor'][:]\n",
    "    zyx2xyz = f['metadata/transformations/bigwarp2neuroglancer'].attrs['zyx2xyz']\n",
    "    # Apply transformations to get Neuroglancer coordinates\n",
    "    # 1. Center points around origin\n",
    "    centered_points = centroids - rotated_cropped_stack_center_shift\n",
    "\n",
    "    # 2. Apply transformation matrix\n",
    "    transformed_centered = np.dot(centered_points, transformation_matrix.T)\n",
    "\n",
    "    # 3. Move to target space and apply shifts\n",
    "    transformed_points = (transformed_centered +\n",
    "                          rotated_stack_center_shift +\n",
    "                          cropped_shift)\n",
    "    if f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor']:\n",
    "        downsampled_factor = f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor'][:]\n",
    "        transformed_points = transformed_points * downsampled_factor\n",
    "\n",
    "    # 4. Convert from ZYX to XYZ if needed\n",
    "    if f['metadata/transformations/bigwarp2neuroglancer'].attrs['zyx2xyz']:\n",
    "        transformed_points = transformed_points[:, ::-1]\n",
    "\n",
    "    # 5. Correct for shift in neuroglancer\n",
    "    transformed_points = transformed_points + ng_shift\n"
   ],
   "id": "3958bfad56fb6e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T12:09:14.224611Z",
     "start_time": "2025-01-30T12:09:14.218609Z"
    }
   },
   "cell_type": "code",
   "source": "lut_path = os.path.join(exp.paths.clem_path, f\"{exp.sample.id}_lut.h5\")",
   "id": "c1f121583f1c0cb9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-30T12:09:52.311325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "error_neurons = []\n",
    "with h5py.File(lut_path, 'w') as hdf5_file:\n",
    "    # Create metadata group with source information\n",
    "    metadata_group = hdf5_file.create_group('metadata')\n",
    "    metadata_group.attrs['source_stack'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM.tif')\n",
    "    metadata_group.attrs['source_mask'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM_mask_filtered.tif')\n",
    "    metadata_group.attrs['sample_id'] = exp.sample.id\n",
    "    metadata_group.attrs['volume_id'] = volume_id\n",
    "    metadata_group.attrs['stack_change'] = stack_change\n",
    "    \n",
    "    # Store transformation parameters\n",
    "    transformations_group = metadata_group.create_group('transformations')\n",
    "    ng2bw_group = transformations_group.create_group('neuroglancer2bigwarp')\n",
    "    ng2bw_group.create_dataset('transformation_matrix', data=transformation_matrix)\n",
    "    ng2bw_group.create_dataset('rotated_cropped_stack_center_shift', data=rotated_cropped_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('rotated_stack_center_shift', data=rotated_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('cropped_shift', data=cropped_shift)\n",
    "    ng2bw_group.create_dataset('ng_shift', data=ng_shift)\n",
    "    ng2bw_group.attrs['zyx2xyz'] = zyx2xyz\n",
    "    ng2bw_group.attrs['downsampled_factor'] = downsampled_factor\n",
    "    \n",
    "    # Create neurons group\n",
    "    neurons_group = hdf5_file.create_group('neurons')\n",
    "    \n",
    "    # Add data for each neuron\n",
    "    for neuron_id in tqdm(range(len(centroids)), desc=\"Finding agglomeration ids and their segments\"):\n",
    "        try:\n",
    "            neuron_group = neurons_group.create_group(f'neuron_{neuron_id}')\n",
    "            neuron_group.create_dataset('em_centroid_bw', data=centroids[neuron_id])\n",
    "            neuron_group.create_dataset('em_centroid_ng', data=transformed_points[neuron_id])\n",
    "            \n",
    "            ng_centroid = transformed_points[neuron_id]\n",
    "            #print(f\"Processing neuron {neuron_id} at position {ng_centroid}\")\n",
    "            \n",
    "            segments_group = neuron_group.create_group('segments')\n",
    "            \n",
    "            # Get agglomeration ID and segments\n",
    "            sr = SubvolumeRequest(sa, volume_id)\n",
    "            centroid_xyz = np.round(ng_centroid).astype(int)\n",
    "            vol = sr.get_subvolume(centroid_xyz, size=[5,5,5], change_stack_id=stack_change)\n",
    "            \n",
    "            # Check if any positive values exist in the volume\n",
    "            if np.any(vol > 0):\n",
    "                agglo_id = int(np.unique(vol[vol>0])[0])\n",
    "                er = EquivalenceRequests(sa, volume_id, stack_change)\n",
    "                agglo_group = er.get_groups(agglo_id)\n",
    "                \n",
    "                for agglo_id, segment_ids in agglo_group.items():\n",
    "                    neuron_group.attrs['agglo_id'] = agglo_id\n",
    "                    neuron_group.create_dataset('agglo_segments', data=segment_ids)\n",
    "            else:\n",
    "                error_neurons.append((neuron_id, \"No agglomeration ID found at position\"))\n",
    "                #print(f\"No agglomeration ID found for neuron {neuron_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_neurons.append((neuron_id, str(e)))\n",
    "            print(f\"Error processing neuron {neuron_id}: {e}\")\n",
    "\n",
    "print(f\"Neurons with errors: {error_neurons}\")\n"
   ],
   "id": "7b6700e69c964b69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   7%|▋         | 623/9462 [32:26<7:53:42,  3.22s/it] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get segments for neuron 0\n",
    "segments = get_neuron_segments(lut_path, 0)\n",
    "print(f\"Segments for neuron 0: {segments}\")\n",
    "\n",
    "# Get segments for agglomeration ID 12345\n",
    "segments = get_segments_by_agglo_id(lut_path, 55233441)\n",
    "print(f\"Segments for agglo_id {55233441}: {segments}\")"
   ],
   "id": "e1e081a0c10767d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finding neurons within OB mask ",
   "id": "b6a8bcfd7453a3f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ob_mask_path = os.path.join(exp.paths.em_path, 'masks', 'ob_mask.tif')\n",
    "neurons_inside = find_neurons_in_mask(lut_path, ob_mask_path)\n",
    "ob_mask = tifffile.imread(ob_mask_path)\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "viewer.add_image(ob_mask)\n",
    "viewer.add_labels(ob_mask)\n"
   ],
   "id": "f6ff1d752c420cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "agglo_id = 61815715\n",
    "with h5py.File(lut_path, 'r') as f:\n",
    "    # Iterate through all neurons to find matching agglo_id\n",
    "    for neuron_name, neuron_group in f['neurons'].items():\n",
    "        if 'agglo_id' in neuron_group.attrs:\n",
    "            if neuron_group.attrs['agglo_id'] == agglo_id:\n",
    "                # Get segments from this neuron\n",
    "                centroid = neuron_group['em_centroid_ng'][:]\n",
    "                print(centroid)\n",
    "                \n",
    " "
   ],
   "id": "76bb79c1c60205c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "               \n",
    "with h5py.File(lut_path, 'r') as f:\n",
    "    transformation = {key: val[()] for key, val in f['metadata/transformations/neuroglancer2bigwarp'].items()}\n",
    "\n",
    "transformation"
   ],
   "id": "c8b9f2a39393af86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "x, y, z = tuple(centroid)\n",
    "print(centroid)\n",
    "print(x, y, z)\n",
    "centroid_ds = np.array((z, y, x)) // 16 - transformation['cropped_shift']\n",
    "print(centroid_ds)\n",
    "viewer.add_points(centroid_ds, size=10)\n"
   ],
   "id": "b0c3c077b5a0f8ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "em_stack = tifffile.imread(\n",
    "    r\"\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\CLEM_Analyses\\CLEM_20220426_RM0008_130hpf_fP1_f3\\fine_aligned_downsampled_4_em_stack.tif\")"
   ],
   "id": "a83af5fdc884f283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "         \n",
    "# For all neurons\n",
    "with h5py.File(lut_path, 'r+') as f:\n",
    "    # Iterate through all neurons to find matching agglo_id\n",
    "    transformation = {key: val[()] for key, val in f['metadata/transformations/neuroglancer2bigwarp'].items()}\n",
    "    for neuron_name, neuron_group in tqdm(f['neurons'].items()):\n",
    "        # Get centroid from this neuron\n",
    "\n",
    "        centroid = neuron_group['em_centroid_ng'][:]\n",
    "        centroid_transformed = centroid[::-1] // 16 - transformation['cropped_shift']\n",
    "\n",
    "        mask_value = ob_mask[tuple(centroid_transformed.astype(int))]\n",
    "\n",
    "        if mask_value == 1:\n",
    "            neuron_group.attrs['in_OB'] = True\n",
    "        elif mask_value == 0:\n",
    "            neuron_group.attrs['in_OB'] = False\n",
    "            \n",
    "     "
   ],
   "id": "8c5613b39b6292ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "       # Read all neurons that are in OB \n",
    "neurons_in_ob = []\n",
    "centroids_in_ob = []\n",
    "centroids_outside_ob = []\n",
    "with h5py.File(lut_path, 'r') as f:\n",
    "    for neuron_name, neuron_group in tqdm(f['neurons'].items()):\n",
    "        # First check if in_OB exists and is True\n",
    "        if neuron_group.attrs.get(\"agglo_id\", False):\n",
    "            if neuron_group.attrs[\"in_OB\"] == True:\n",
    "                neurons_in_ob.append(neuron_group.attrs['agglo_id'])\n",
    "                centroids_in_ob.append(neuron_group['em_centroid_ng'][:])\n",
    "            else:\n",
    "                centroids_outside_ob.append(neuron_group['em_centroid_ng'][:])\n",
    "\n",
    "print(len(centroids_in_ob))\n",
    "print(len(centroids_outside_ob))\n",
    "print(np.stack(centroids_in_ob[:10]))\n",
    "centroids_in_ob_transformed = [centroid[::-1] for centroid in centroids_in_ob]\n",
    "print(centroids_in_ob_transformed[:10])"
   ],
   "id": "f33443020e465168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "viewer.add_points(np.array([centroid[::-1] for centroid in centroids_in_ob]) // 16 - transformation['cropped_shift'],\n",
    "                  size=10, face_color=\"green\")\n",
    "viewer.add_points(\n",
    "    np.array([centroid[::-1] for centroid in centroids_outside_ob]) // 16 - transformation['cropped_shift'], size=10,\n",
    "    face_color=\"red\")"
   ],
   "id": "b542445fe58468e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Adding LM stack centroids\n",
    "exp.paths.clem_path\n",
    "os.listdir(exp.paths.clem_path)"
   ],
   "id": "c72d52eb2f4e4290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load an existing interpolator \n",
    "with open(exp.paths.clem_path + '/' + f'{exp.sample.id}_em2lm_interpolator.dill', 'rb') as f:\n",
    "    rbf_interpolator = dill.load(f)\n",
    "\n",
    "with h5py.File(em_centroids_path, 'r') as f:\n",
    "    # Get centroids coordinates\n",
    "    data = f['centroids/coordinates'][:]\n",
    "    em_centroids = data[:, 1:]\n",
    "\n",
    "# Compare predictions\n",
    "lm_centroids_proxy = rbf_interpolator(em_centroids)\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_points(em_centroids, face_color=\"green\")\n",
    "viewer.add_points(lm_centroids_proxy, face_color=\"red\")"
   ],
   "id": "58b4a3bb47618841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "os.listdir(os.path.join(exp.paths.anatomy_path, 'processed'))\n",
    "lm_stack = load_tiff_as_hyperstack(os.path.join(exp.paths.anatomy_path, 'processed',\n",
    "                                                'flipped_upsampled_clahe_20220426_RM0008_130hpf_fP1_f3_anatomyGFRF_001_.tif'))\n",
    "lm_stack.shape"
   ],
   "id": "bc518de5c02c9b08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "viewer.add_image(lm_stack[:, 0], name=\"lm_stack\")\n",
    "with h5py.File(lut_path, 'r+') as f:\n",
    "    # Create group and dataset\n",
    "\n",
    "    f['metadata']['transformations'].create_group('em2lmstack_bigwarp')\n",
    "    interpolator_path = os.path.join(exp.paths.clem_path, f'{exp.sample.id}_em2lm_interpolator.dill')\n",
    "    f['metadata']['transformations']['em2lmstack_bigwarp'].create_dataset('rfb_interpolator_path',\n",
    "                                                                          data=interpolator_path)"
   ],
   "id": "ff5422f77e78534d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "test_point = em_centroids[10:20]\n",
    "print(test_point)\n",
    "test_lm_point = rbf_interpolator(test_point)\n",
    "\n",
    "viewer.add_points(test_point, face_color=\"blue\")\n",
    "viewer.add_points(test_lm_point, face_color=\"cyan\")\n",
    "with h5py.File(lut_minimal_path, 'r+') as f:\n",
    "    for ii, (neuron, neuron_group) in enumerate(f['neurons'].items()):\n",
    "        f['neurons'][neuron].create_dataset('lm_centroid_bw', data=lm_centroids_proxy[ii])\n",
    "\n",
    "# LM bw to LM raw transformation\n",
    "\n",
    "# Assign IN information\n",
    "\n",
    "mask_colored_stack_c0_path = r\"\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\2P_RawData\\2022-04-26\\f3\\anatomy\\masks\\mask_manderscoeff_c0_20220426_RM0008_130hpf_fP1_f3_anatomyGFRF_001_.tif\"\n",
    "mask_colored_stack_c1_path = r\"\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\2P_RawData\\2022-04-26\\f3\\anatomy\\masks\\mask_manderscoeff_c1_20220426_RM0008_130hpf_fP1_f3_anatomyGFRF_001_.tif\"\n",
    "\n",
    "mask_colored_stack_c0 = tifffile.imread(mask_colored_stack_c0_path)\n",
    "mask_colored_stack_c1 = tifffile.imread(mask_colored_stack_c1_path)\n",
    "\n",
    "viewer.add_image(mask_colored_stack_c0, name=\"mask_colored_stack_c0\")\n",
    "viewer.add_image(mask_colored_stack_c1, name=\"mask_colored_stack_c1\")\n",
    "r_in_centroids = []\n",
    "r_out_centroids = []\n",
    "\n",
    "r_pos_centroids = []\n",
    "r_neg_centroids = []\n",
    "c1_threshold = 0.8\n",
    "\n",
    "# Get mask dimensions for bounds checking\n",
    "z_max, y_max, x_max = mask_colored_stack_c1.shape\n",
    "\n",
    "for centroid in lm_centroids_proxy:\n",
    "    z, y, x = centroid\n",
    "    int_centroid = centroid.astype(int)\n",
    "    z_int, y_int, x_int = int_centroid\n",
    "\n",
    "    # Check if centroid is within mask bounds\n",
    "    if (z_int < 0 or y_int < 0 or x_int < 0 or\n",
    "            z_int >= z_max or y_int >= y_max or x_int >= x_max):\n",
    "        r_out_centroids.append(centroid)\n",
    "        continue\n",
    "\n",
    "    # Get coefficient and classify centroid\n",
    "    try:\n",
    "        c1_coeff = mask_colored_stack_c1[z_int, y_int, x_int]\n",
    "        r_in_centroids.append(centroid)\n",
    "\n",
    "        if c1_coeff > c1_threshold:\n",
    "            r_pos_centroids.append(centroid)\n",
    "        else:\n",
    "            r_neg_centroids.append(centroid)\n",
    "\n",
    "    except IndexError:\n",
    "        r_out_centroids.append(centroid)"
   ],
   "id": "be3921448306c3a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(lm_stack[:, 1], blending=\"additive\", name=\"lm_stack\")\n",
    "viewer.add_image(mask_colored_stack_c1, blending=\"additive\", name=\"c1_mask\")\n",
    "viewer.add_points(r_in_centroids, face_color=\"green\", name=\"r_in_centroids\")\n",
    "viewer.add_points(r_out_centroids, face_color=\"red\", name=\"r_out_centroids\")\n",
    "viewer.add_points(r_pos_centroids, face_color=\"cyan\", name=\"r_pos_centroids\")\n",
    "viewer.add_points(r_neg_centroids, face_color=\"magenta\", name=\"r_neg_centroids\")\n",
    "\n",
    "\n",
    "lm_centroids_stored = []\n",
    "em_centroids_stored = []\n",
    "ng_centroids_stored = []\n",
    "outside_lm_points = []\n",
    "with h5py.File(lut_minimal_path, 'r+') as f:\n",
    "    for neuron, neuron_group in f['neurons'].items():\n",
    "        if 'lm_centroid_bw' in neuron_group:\n",
    "            lm_centroids_stored.append(list(neuron_group['lm_centroid_bw'][:]))\n",
    "            em_centroids_stored.append(list(neuron_group['em_centroid_bw'][:]))\n",
    "            ng_centroids_stored.append(list(neuron_group['em_centroid_ng'][:]))\n",
    "\n",
    "viewer.add_points(lm_centroids_stored, name=\"lm_centroids_stored\")\n",
    "viewer.add_points(em_centroids_stored, name=\"em_centroids_stored\")"
   ],
   "id": "215f1440835a384f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Generate a color cycle using hsv colormap which gives better color separation\n",
    "num_colors = 10000  # Increased number of colors\n",
    "colors = plt.cm.coolwarm(np.linspace(0, 1, num_colors))\n",
    "\n",
    "# Add points with the larger color cycle\n",
    "properties = {\n",
    "    'point_index': np.arange(len(lm_centroids_stored))\n",
    "}\n",
    "\n",
    "viewer.add_points(\n",
    "    lm_centroids_stored,\n",
    "    name=\"lm_centroids_stored\",\n",
    "    properties=properties,\n",
    "    face_color='point_index',\n",
    "    face_color_cycle=colors,\n",
    "    size=10\n",
    ")\n",
    "\n",
    "viewer.add_points(\n",
    "    em_centroids_stored,\n",
    "    name=\"em_centroids_stored\",\n",
    "    properties=properties,\n",
    "    face_color='point_index',\n",
    "    face_color_cycle=colors,\n",
    "    size=10\n",
    ")\n",
    "\n",
    "viewer.add_points(\n",
    "    ng_centroids_stored,\n",
    "    name=\"ng_centroids_stored\",\n",
    "    properties=properties,\n",
    "    face_color='point_index',\n",
    "    face_color_cycle=colors,\n",
    "    size=100\n",
    ")\n"
   ],
   "id": "4ef8a2c49a21a62a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "outside_lm_points = []\n",
    "\n",
    "# Get mask dimensions for bounds checking\n",
    "z_max, y_max, x_max = mask_colored_stack_c0.shape\n",
    "\n",
    "with h5py.File(lut_minimal_path, 'r+') as f:\n",
    "    for neuron, neuron_group in f['neurons'].items():\n",
    "        if 'lm_centroid_bw' in neuron_group:\n",
    "            # Get centroid coordinates\n",
    "            lm_centroid = neuron_group['lm_centroid_bw'][:].round().astype(int)\n",
    "            z, y, x = lm_centroid\n",
    "\n",
    "            # Check if centroid is within mask bounds\n",
    "            if (z < 0 or y < 0 or x < 0 or\n",
    "                    z >= z_max or y >= y_max or x >= x_max):\n",
    "                outside_lm_points.append(lm_centroid)\n",
    "                neuron_group.attrs['g_coeff'] = np.nan\n",
    "                neuron_group.attrs['r_coeff'] = np.nan\n",
    "                continue\n",
    "\n",
    "            # Get coefficients from masks at centroid position\n",
    "            try:\n",
    "                c0_coeff = mask_colored_stack_c0[z, y, x]\n",
    "                neuron_group.attrs['g_coeff'] = c0_coeff\n",
    "            except IndexError:\n",
    "                outside_lm_points.append(lm_centroid)\n",
    "                neuron_group.attrs['g_coeff'] = np.nan\n",
    "\n",
    "            try:\n",
    "                c1_coeff = mask_colored_stack_c1[z, y, x]\n",
    "                neuron_group.attrs['r_coeff'] = c1_coeff\n",
    "            except IndexError:\n",
    "                outside_lm_points.append(lm_centroid)\n",
    "                neuron_group.attrs['r_coeff'] = np.nan\n",
    "\n",
    "INs = get_neurons_with_attribute(lut_minimal_path, \"r_coeff\", 0.8, \">\")\n",
    "print(INs)\n",
    "\n",
    "viewer.add_points(outside_lm_points, face_color=\"magenta\")\n",
    "outside_lm_points.shape\n"
   ],
   "id": "a0854c245adc9b25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "43960bd0949c928e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
