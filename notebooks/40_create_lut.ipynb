{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-23T09:59:59.799966Z",
     "start_time": "2025-01-23T09:59:59.792960Z"
    }
   },
   "source": [
    "# This script should take the em_centroids h5_file and query the agglo_id\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from brainmaps_api_fcn.equivalence_requests import EquivalenceRequests\n",
    "from brainmaps_api_fcn.subvolume_requests import SubvolumeRequest\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scripts.config_model import save_experiment_config, tree\n",
    "from scripts.sample_db import SampleDB\n",
    "from scripts.utils.image_utils import load_tiff_as_hyperstack, save_array_as_hyperstack_tiff\n",
    "\n",
    "from google.auth import exceptions\n",
    "\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:04:36.050467Z",
     "start_time": "2025-01-23T10:04:36.037463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Get the agglomerated ID that contains EM centroid\n",
    "def get_agglo_group_from_point(sa, volume_id, stack_change, centroid_xyz):\n",
    "    sr = SubvolumeRequest(sa, volume_id)\n",
    "    vol = sr.get_subvolume(centroid_xyz, size= [1,1,1], change_stack_id = stack_change)\n",
    "    agglo_id  =  int(np.unique(vol[vol>0])[0])\n",
    "        #\n",
    "        # print(f\"ID found: {agglo_id}\")\n",
    "\n",
    "    er = EquivalenceRequests(sa, volume_id, stack_change)\n",
    "    group = er.get_groups(agglo_id)\n",
    "    return group\n",
    "\n",
    "\n",
    "def get_fresh_client():\n",
    "    return bigquery.Client(project=\"aggloproofreading\")\n",
    "\n",
    "\n",
    "def get_agglo_group_with_retry(sa, volume_id, stack_change, centroid_xyz, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            sr = SubvolumeRequest(sa, volume_id)\n",
    "            vol = sr.get_subvolume(centroid_xyz, size=[1,1,1], change_stack_id=stack_change)\n",
    "            agglo_id = int(np.unique(vol[vol>0])[0])\n",
    "            \n",
    "            er = EquivalenceRequests(sa, volume_id, stack_change)\n",
    "            group = er.get_groups(agglo_id)\n",
    "            return group\n",
    "        except exceptions.RefreshError:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            print(f\"Try {attempt+1}/{max_retries}\")\n",
    "            time.sleep(attempt)"
   ],
   "id": "2e8b33619301dc11",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:10:59.771353Z",
     "start_time": "2025-01-23T09:10:55.567747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = bigquery.Client(project=\"aggloproofreading\")\n",
    "\n",
    "# Step 1: Load the sample database\n",
    "db_path = r'\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\sample_db.csv'\n",
    "sample_db = SampleDB()\n",
    "sample_db.load(db_path)\n",
    "# Step 2: Load experiment configuration\n",
    "sample_id = '20220426_RM0008_130hpf_fP1_f3'\n",
    "exp = sample_db.get_sample(sample_id)\n",
    "\n",
    "# TODO: add paths to sample\n",
    "# Input: volume data\n",
    "sa = r\"\\\\tungsten-nas.fmi.ch\\tungsten\\scratch\\gfriedri\\montruth\\Reconstruction\\fmi-friedrich-4dd3f21e665d.json\"\n",
    "volume_id = r\"280984173682:montano_rm2_ngff:raw_230701_seg_240316fb\"\n",
    "stack_change = \"240705d_rsg9_spl\""
   ],
   "id": "2b4052031234400e",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:10:59.781346Z",
     "start_time": "2025-01-23T09:10:59.774349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "em_centroids_path = os.path.join(exp.paths.em_path, f\"{exp.sample.id}_em_centroids.h5\")\n"
   ],
   "id": "2e5b628425194ebd",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:11:02.037386Z",
     "start_time": "2025-01-23T09:11:01.987380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read centroids from HDF5 file and do transformation to neuroglancer\n",
    "with h5py.File(em_centroids_path, 'r') as f:\n",
    "    # Get centroids coordinates\n",
    "    data = f['centroids/coordinates'][:]\n",
    "    centroids = data[:,1:]\n",
    "    \n",
    "    # Get transformation parameters\n",
    "    transformation_matrix = f['metadata/transformations/bigwarp2neuroglancer/transformation_matrix'][:]\n",
    "    rotated_cropped_stack_center_shift = f['metadata/transformations/bigwarp2neuroglancer/rotated_cropped_stack_center_shift'][:]\n",
    "    rotated_stack_center_shift = f['metadata/transformations/bigwarp2neuroglancer/rotated_stack_center_shift'][:]\n",
    "    cropped_shift = f['metadata/transformations/bigwarp2neuroglancer/cropped_shift'][:]\n",
    "    ng_shift = f['metadata/transformations/bigwarp2neuroglancer/ng_shift'][:]\n",
    "    downsampled_factor = f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor'][:]\n",
    "    zyx2xyz = f['metadata/transformations/bigwarp2neuroglancer'].attrs['zyx2xyz']\n",
    "    # Apply transformations to get Neuroglancer coordinates\n",
    "    # 1. Center points around origin\n",
    "    centered_points = centroids - rotated_cropped_stack_center_shift\n",
    "    \n",
    "    # 2. Apply transformation matrix\n",
    "    transformed_centered = np.dot(centered_points, transformation_matrix.T)\n",
    "    \n",
    "    # 3. Move to target space and apply shifts\n",
    "    transformed_points = (transformed_centered + \n",
    "                         rotated_stack_center_shift + \n",
    "                         cropped_shift)\n",
    "    if f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor']:\n",
    "        downsampled_factor = f['metadata/transformations/bigwarp2neuroglancer/downsampled_factor'][:]\n",
    "        transformed_points = transformed_points*downsampled_factor\n",
    "    \n",
    "    # 4. Convert from ZYX to XYZ if needed\n",
    "    if f['metadata/transformations/bigwarp2neuroglancer'].attrs['zyx2xyz']:\n",
    "        transformed_points = transformed_points[:, ::-1]\n",
    "    \n",
    "    # 5. Correct for shift in neuroglancer\n",
    "    transformed_points = transformed_points + ng_shift\n",
    "\n"
   ],
   "id": "3958bfad56fb6e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-23T10:11:12.528315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lut_path = os.path.join(exp.paths.clem_path, f\"{exp.sample.id}_lut.h5\")\n",
    "error_neurons = []\n",
    "with h5py.File(lut_path, 'w') as hdf5_file:\n",
    "    # Create metadata group with source information\n",
    "    metadata_group = hdf5_file.create_group('metadata')\n",
    "    metadata_group.attrs['source_stack'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM.tif')\n",
    "    metadata_group.attrs['source_mask'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM_mask_filtered.tif')\n",
    "    metadata_group.attrs['sample_id'] = exp.sample.id\n",
    "    metadata_group.attrs['volume_id'] = volume_id\n",
    "    metadata_group.attrs['stack_change'] = stack_change\n",
    "    \n",
    "    # Store transformation parameters\n",
    "    transformations_group = metadata_group.create_group('transformations')\n",
    "    ng2bw_group = transformations_group.create_group('neuroglancer2bigwarp')\n",
    "    ng2bw_group.create_dataset('transformation_matrix', data=transformation_matrix)\n",
    "    ng2bw_group.create_dataset('rotated_cropped_stack_center_shift', data=rotated_cropped_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('rotated_stack_center_shift', data=rotated_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('cropped_shift', data=cropped_shift)\n",
    "    ng2bw_group.create_dataset('ng_shift', data=ng_shift)\n",
    "    ng2bw_group.attrs['zyx2xyz'] = zyx2xyz\n",
    "    \n",
    "    # Create neurons group\n",
    "    neurons_group = hdf5_file.create_group('neurons')\n",
    "    \n",
    "    # Add data for each neuron\n",
    "    # Step 2: Query and update segment information\n",
    "    for neuron_id in tqdm(range(len(centroids)), desc=\"Finding agglomeration ids and their segments\"):\n",
    "        \n",
    "        neuron_group = neurons_group.create_group(f'neuron_{neuron_id}')\n",
    "        neuron_group.create_dataset('em_centroid_bw', data=centroids[neuron_id])\n",
    "        neuron_group.create_dataset('em_centroid_ng', data=transformed_points[neuron_id])\n",
    "        \n",
    "        segments_group = neuron_group.create_group('segments')\n",
    "        \n",
    "        try:\n",
    "            # Get agglomeration ID and segments\n",
    "            agglo_group = get_agglo_group_with_retry(sa, volume_id, stack_change, \n",
    "                                                    np.round(transformed_points[neuron_id]).astype(int))\n",
    "            neuron_group.attrs['agglo_id'] = list(agglo_group.keys())[0]\n",
    "            \n",
    "            for agglo_id, segment_ids in agglo_group.items():\n",
    "   \n",
    "                # Construct SQL query for the current batch\n",
    "                query = f\"\"\"\n",
    "                SELECT id AS segment_id,\n",
    "                       bbox.start.x AS start_x,\n",
    "                       bbox.start.y AS start_y,\n",
    "                       bbox.start.z AS start_z,\n",
    "                       bbox.size.x AS size_x,\n",
    "                       bbox.size.y AS size_y,\n",
    "                       bbox.size.z AS size_z,\n",
    "                       num_voxels  \n",
    "                FROM `fmi-friedrich.ruth_ob.raw_230701_seg_240316fb_objinfo` \n",
    "                WHERE id IN {tuple(segment_ids)}\n",
    "                \"\"\"\n",
    "\n",
    "                # Execute the query\n",
    "                query_job = client.query(query)\n",
    "\n",
    "                # Store results in HDF5\n",
    "                for row in query_job.result():\n",
    "                    segment_group = segments_group.create_group(str(row['segment_id']))\n",
    "                    segment_group.create_dataset('bbox_start', data=[row['start_x'], row['start_y'], row['start_z']])\n",
    "                    segment_group.create_dataset('bbox_size', data=[row['size_x'], row['size_y'], row['size_z']])\n",
    "                    segment_group.create_dataset('num_voxels', data=row['num_voxels'])\n",
    "                    segment_group.attrs['axes_order'] = \"xyz\"\n",
    "\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_neurons.append(neuron_id)\n",
    "            print(f\"Error processing neuron {neuron_id}: {e}\")\n",
    "\n",
    "\n"
   ],
   "id": "7b6700e69c964b69",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 0/9462 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:43:56.444425Z",
     "start_time": "2025-01-23T09:40:29.450600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lut_path = os.path.join(exp.paths.clem_path, f\"{exp.sample.id}_lut.h5\")\n",
    "error_neurons = []\n",
    "with h5py.File(lut_path, 'w') as hdf5_file:\n",
    "    # Create metadata group with source information\n",
    "    metadata_group = hdf5_file.create_group('metadata')\n",
    "    metadata_group.attrs['source_stack'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM.tif')\n",
    "    metadata_group.attrs['source_mask'] = os.path.join(exp.paths.em_path, '20220426_RM0008_130hpf_fP1_f3_fine_aligned_downsampled_16_em_stack_cropped_woResin_rough_rotated_to_LM_mask_filtered.tif')\n",
    "    metadata_group.attrs['sample_id'] = exp.sample.id\n",
    "    metadata_group.attrs['volume_id'] = volume_id\n",
    "    metadata_group.attrs['stack_change'] = stack_change\n",
    "    \n",
    "    # Store transformation parameters\n",
    "    transformations_group = metadata_group.create_group('transformations')\n",
    "    ng2bw_group = transformations_group.create_group('neuroglancer2bigwarp')\n",
    "    ng2bw_group.create_dataset('transformation_matrix', data=transformation_matrix)\n",
    "    ng2bw_group.create_dataset('rotated_cropped_stack_center_shift', data=rotated_cropped_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('rotated_stack_center_shift', data=rotated_stack_center_shift)\n",
    "    ng2bw_group.create_dataset('cropped_shift', data=cropped_shift)\n",
    "    ng2bw_group.create_dataset('ng_shift', data=ng_shift)\n",
    "    ng2bw_group.attrs['zyx2xyz'] = zyx2xyz\n",
    "    \n",
    "    # Create neurons group\n",
    "    neurons_group = hdf5_file.create_group('neurons')\n",
    "    \n",
    "    # Add data for each neuron\n",
    "    # Step 2: Query and update segment information\n",
    "    for neuron_id in tqdm(range(len(centroids)), desc=\"Finding agglomeration ids and their segments\"):\n",
    "        neuron_group = neurons_group.create_group(f'neuron_{neuron_id}')\n",
    "        neuron_group.create_dataset('em_centroid_bw', data=centroids[neuron_id])\n",
    "        neuron_group.create_dataset('em_centroid_ng', data=transformed_points[neuron_id])\n",
    "        \n",
    "        segments_group = neuron_group.create_group('segments')\n",
    "        \n",
    "        try:\n",
    "            # Get agglomeration ID and segments\n",
    "            agglo_group = get_agglo_group_from_point(sa, volume_id, stack_change, \n",
    "                                                    np.round(transformed_points[neuron_id]).astype(int))\n",
    "            neuron_group.attrs['agglo_id'] = list(agglo_group.keys())[0]\n",
    "            \n",
    "            # Get segment IDs from agglo_group\n",
    "            for agglo_id, segment_ids in agglo_group.items():\n",
    "   \n",
    "                # Construct SQL query for the current batch\n",
    "                query = f\"\"\"\n",
    "                SELECT id AS segment_id,\n",
    "                       bbox.start.x AS start_x,\n",
    "                       bbox.start.y AS start_y,\n",
    "                       bbox.start.z AS start_z,\n",
    "                       bbox.size.x AS size_x,\n",
    "                       bbox.size.y AS size_y,\n",
    "                       bbox.size.z AS size_z,\n",
    "                       num_voxels  \n",
    "                FROM `fmi-friedrich.ruth_ob.raw_230701_seg_240316fb_objinfo` \n",
    "                WHERE id IN {tuple(segment_ids)}\n",
    "                \"\"\"\n",
    "\n",
    "                # Execute the query\n",
    "                query_job = client.query(query)\n",
    "\n",
    "                # Store results in HDF5\n",
    "                for row in query_job.result():\n",
    "                    segment_group = segments_group.create_group(str(row['segment_id']))\n",
    "                    segment_group.create_dataset('bbox_start', data=[row['start_x'], row['start_y'], row['start_z']])\n",
    "                    segment_group.create_dataset('bbox_size', data=[row['size_x'], row['size_y'], row['size_z']])\n",
    "                    segment_group.create_dataset('num_voxels', data=row['num_voxels'])\n",
    "                    segment_group.attrs['axes_order'] = \"xyz\"\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_neurons.append(neuron_id)\n",
    "            print(f\"Error processing neuron {neuron_id}: {e}\")\n",
    "\n",
    "\n"
   ],
   "id": "27286af7214a4578",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 13/9462 [01:02<9:47:02,  3.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 12: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 23/9462 [01:52<11:48:39,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 22: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 31/9462 [02:28<10:30:40,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 30: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 34/9462 [02:40<9:54:51,  3.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 33: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 38/9462 [02:55<8:44:16,  3.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 37: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 39/9462 [02:58<8:01:04,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 38: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 41/9462 [03:05<8:37:38,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 40: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 44/9462 [03:16<8:06:34,  3.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 43: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 45/9462 [03:17<6:08:48,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 44: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   0%|          | 46/9462 [03:19<5:47:54,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 45: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   1%|          | 48/9462 [03:24<5:52:58,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing neuron 47: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding agglomeration ids and their segments:   1%|          | 48/9462 [03:26<11:16:06,  4.31s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter only dendrites\n",
    "# Filter only OB\n",
    "# Create convex hulls \n",
    "\n"
   ],
   "id": "43960bd0949c928e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
